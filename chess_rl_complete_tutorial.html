<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chess RL: Complete Tutorial - From Random to AlphaZero</title>
    <style>
        /* ===== GLOBAL STYLES ===== */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }

        /* ===== SIDEBAR NAVIGATION ===== */
        .sidebar {
            position: fixed;
            left: 0;
            top: 0;
            width: 280px;
            height: 100vh;
            background: linear-gradient(180deg, #2c3e50 0%, #34495e 100%);
            color: white;
            overflow-y: auto;
            z-index: 1000;
            box-shadow: 4px 0 10px rgba(0,0,0,0.1);
        }

        .sidebar-header {
            padding: 30px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-bottom: 3px solid rgba(255,255,255,0.1);
        }

        .sidebar-header h1 {
            font-size: 1.4em;
            margin-bottom: 10px;
        }

        .progress-indicator {
            background: rgba(255,255,255,0.2);
            padding: 8px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            text-align: center;
            margin-top: 10px;
        }

        .nav-section {
            padding: 10px 0;
        }

        .nav-section h3 {
            padding: 15px 20px 10px;
            font-size: 0.75em;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: #95a5a6;
        }

        .sidebar nav ul {
            list-style: none;
        }

        .sidebar nav ul li {
            border-left: 3px solid transparent;
            transition: all 0.3s ease;
        }

        .sidebar nav ul li.current {
            border-left-color: #667eea;
            background: rgba(102, 126, 234, 0.1);
        }

        .sidebar nav ul li a {
            display: block;
            padding: 12px 20px;
            color: #ecf0f1;
            text-decoration: none;
            transition: all 0.2s ease;
        }

        .sidebar nav ul li a:hover {
            background: rgba(255,255,255,0.1);
            padding-left: 25px;
        }

        .sidebar nav ul li.current a {
            color: #fff;
            font-weight: 600;
        }

        .sidebar nav ul ul {
            background: rgba(0,0,0,0.1);
        }

        .sidebar nav ul ul li a {
            padding-left: 40px;
            font-size: 0.9em;
        }

        .coming-soon {
            opacity: 0.5;
            font-style: italic;
        }

        .phase-badge {
            display: inline-block;
            background: #667eea;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 0.7em;
            margin-left: 8px;
        }

        /* ===== MAIN CONTENT ===== */
        .content {
            margin-left: 280px;
            min-height: 100vh;
        }

        .content-inner {
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px;
            background: white;
            min-height: 100vh;
        }

        /* ===== TYPOGRAPHY ===== */
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin: 30px 0 20px;
            border-bottom: 4px solid #667eea;
            padding-bottom: 15px;
        }

        h2 {
            color: #34495e;
            font-size: 2em;
            margin: 50px 0 20px;
            border-left: 6px solid #667eea;
            padding-left: 20px;
            background: linear-gradient(90deg, #f8f9fa 0%, white 100%);
            padding: 15px 20px;
            border-radius: 4px;
        }

        h3 {
            color: #555;
            font-size: 1.5em;
            margin: 30px 0 15px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 8px;
        }

        h4 {
            color: #666;
            font-size: 1.2em;
            margin: 20px 0 10px;
        }

        p {
            margin: 15px 0;
        }

        ul, ol {
            margin: 15px 0 15px 30px;
        }

        li {
            margin: 8px 0;
        }

        /* ===== CODE STYLES ===== */
        code {
            background-color: #f4f4f4;
            padding: 2px 8px;
            border-radius: 4px;
            font-family: 'Monaco', 'Courier New', Courier, monospace;
            color: #e74c3c;
            font-size: 0.9em;
        }

        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            line-height: 1.5;
            border-left: 4px solid #667eea;
            margin: 20px 0;
        }

        pre code {
            background-color: transparent;
            color: #ecf0f1;
            padding: 0;
        }

        /* ===== CALLOUT BOXES ===== */
        .learning-objective {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(102, 126, 234, 0.3);
        }

        .learning-objective h3, .learning-objective h4 {
            color: white;
            border: none;
            margin-top: 0;
            padding-bottom: 0;
        }

        .key-insight {
            background-color: #e8f4f8;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin: 25px 0;
            border-radius: 4px;
        }

        .key-insight strong {
            color: #2980b9;
        }

        .warning {
            background-color: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 25px 0;
            border-radius: 4px;
        }

        .success {
            background-color: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 25px 0;
            border-radius: 4px;
        }

        .checkpoint {
            background-color: #f0f0ff;
            border: 2px solid #667eea;
            padding: 25px;
            margin: 30px 0;
            border-radius: 8px;
        }

        .checkpoint h4 {
            color: #667eea;
            margin-top: 0;
            font-size: 1.3em;
        }

        .diagram {
            background-color: #f9f9f9;
            border: 2px solid #ddd;
            padding: 20px;
            margin: 25px 0;
            font-family: 'Courier New', Courier, monospace;
            border-radius: 6px;
            overflow-x: auto;
        }

        .code-explanation {
            background-color: #fffef0;
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 15px 0 25px 0;
            font-size: 0.95em;
        }

        /* ===== PHASE SECTIONS ===== */
        .phase {
            margin: 60px 0;
            border: 3px solid #667eea;
            border-radius: 12px;
            overflow: hidden;
        }

        .phase summary {
            font-size: 1.8em;
            font-weight: bold;
            cursor: pointer;
            padding: 25px 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            user-select: none;
            list-style: none;
            transition: all 0.3s ease;
        }

        .phase summary::-webkit-details-marker {
            display: none;
        }

        .phase summary::before {
            content: '‚ñ∂ ';
            display: inline-block;
            transition: transform 0.3s ease;
        }

        .phase[open] summary::before {
            transform: rotate(90deg);
        }

        .phase summary:hover {
            background: linear-gradient(135deg, #5568d3 0%, #6941a0 100%);
        }

        .phase-content {
            padding: 40px;
            background: white;
        }

        .phase-coming-soon {
            opacity: 0.6;
        }

        .phase-coming-soon summary {
            background: linear-gradient(135deg, #95a5a6 0%, #7f8c8d 100%);
        }

        /* ===== NAVIGATION AIDS ===== */
        .section-nav {
            display: flex;
            justify-content: space-between;
            margin: 40px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
        }

        .section-nav a {
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 6px;
            transition: all 0.3s ease;
            font-weight: 600;
        }

        .section-nav a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(102, 126, 234, 0.3);
        }

        .breadcrumb {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 20px;
        }

        .breadcrumb a {
            color: #667eea;
            text-decoration: none;
        }

        /* ===== TABLES ===== */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background-color: #667eea;
            color: white;
            font-weight: bold;
        }

        tr:hover {
            background-color: #f5f5f5;
        }

        /* ===== CHECKLIST ===== */
        ul.checklist {
            list-style: none;
            padding-left: 0;
        }

        ul.checklist li:before {
            content: "‚úÖ ";
            margin-right: 10px;
        }

        ul.checklist-pending li:before {
            content: "üìÖ ";
        }

        /* ===== DAY MARKERS ===== */
        .day-marker {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 8px 20px;
            border-radius: 25px;
            font-weight: bold;
            font-size: 0.95em;
            margin: 15px 0;
            box-shadow: 0 2px 4px rgba(102, 126, 234, 0.3);
        }

        .day-marker.complete {
            background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
        }

        .day-marker.current {
            background: linear-gradient(135deg, #ffc107 0%, #ff9800 100%);
        }

        /* ===== PROGRESS BAR ===== */
        .progress-bar {
            background: #e0e0e0;
            height: 30px;
            border-radius: 15px;
            margin: 20px 0;
            position: relative;
            overflow: hidden;
            box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
        }

        .progress-fill {
            background: linear-gradient(90deg, #28a745 0%, #20c997 100%);
            height: 100%;
            border-radius: 15px;
            transition: width 0.5s ease;
        }

        .progress-label {
            position: absolute;
            width: 100%;
            text-align: center;
            line-height: 30px;
            color: white;
            font-weight: bold;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
            z-index: 1;
        }

        /* ===== PRINT STYLES ===== */
        @media print {
            .sidebar {
                display: none;
            }

            .content {
                margin-left: 0;
            }

            .phase {
                page-break-inside: avoid;
                border: 1px solid #ddd;
            }

            .phase summary {
                background: #667eea;
                -webkit-print-color-adjust: exact;
                print-color-adjust: exact;
            }

            details {
                display: block !important;
            }

            details[open] {
                display: block !important;
            }
        }

        /* ===== RESPONSIVE ===== */
        @media (max-width: 768px) {
            .sidebar {
                transform: translateX(-100%);
                transition: transform 0.3s ease;
            }

            .sidebar.mobile-open {
                transform: translateX(0);
            }

            .content {
                margin-left: 0;
            }

            .content-inner {
                padding: 20px;
            }

            .section-nav {
                flex-direction: column;
                gap: 10px;
            }
        }

        /* ===== INTRO SECTION ===== */
        .hero {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            margin: -40px -40px 40px;
            border-radius: 0 0 20px 20px;
            text-align: center;
        }

        .hero h1 {
            color: white;
            border: none;
            margin: 0;
            padding: 0;
            font-size: 3em;
        }

        .hero p {
            font-size: 1.3em;
            margin: 20px 0;
            opacity: 0.95;
        }

        /* ===== GLOSSARY ===== */
        .glossary-term {
            margin: 20px 0;
            padding: 15px;
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            border-radius: 4px;
        }

        .glossary-term dt {
            font-weight: bold;
            color: #667eea;
            font-size: 1.1em;
            margin-bottom: 5px;
        }

        .glossary-term dd {
            margin-left: 0;
            color: #555;
        }
    </style>
</head>
<body>
    <!-- SIDEBAR NAVIGATION -->
    <aside class="sidebar">
        <div class="sidebar-header">
            <h1>üèÜ Chess RL Complete Tutorial</h1>
            <div class="progress-indicator">
                Phase 3 Complete ‚Ä¢ 60% Progress
            </div>
        </div>

        <div class="nav-section">
            <h3>Introduction</h3>
            <nav>
                <ul>
                    <li><a href="#intro">Welcome & Overview</a></li>
                    <li><a href="#journey">The Learning Journey</a></li>
                </ul>
            </nav>
        </div>

        <div class="nav-section">
            <h3>Tutorial Phases</h3>
            <nav>
                <ul>
                    <li><a href="#phase0">üìç Phase 0: Random Player</a></li>
                    <li><a href="#phase1">üìç Phase 1: Minimax & Evaluation</a></li>
                    <li><a href="#phase2">üìç Phase 2: Monte Carlo Tree Search</a></li>
                    <li class="current">
                        <a href="#phase3">üìç Phase 3: Neural Networks</a>
                        <ul>
                            <li><a href="#phase3-day1">Days 1-2: Architecture</a></li>
                            <li><a href="#phase3-day3">Days 3-4: Dataset</a></li>
                            <li><a href="#phase3-day8">Days 8-10: Training</a></li>
                        </ul>
                    </li>
                    <li class="coming-soon"><a href="#phase4">üìç Phase 4: Self-Play RL</a></li>
                    <li class="coming-soon"><a href="#phase5">üìç Phase 5: Scaling Up</a></li>
                </ul>
            </nav>
        </div>

        <div class="nav-section">
            <h3>Reference</h3>
            <nav>
                <ul>
                    <li><a href="#glossary">Complete Glossary</a></li>
                    <li><a href="#resources">Learning Resources</a></li>
                    <li><a href="#timeline">AlphaZero Timeline</a></li>
                </ul>
            </nav>
        </div>
    </aside>

    <!-- MAIN CONTENT -->
    <main class="content">
        <div class="content-inner">

            <!-- HERO SECTION -->
            <div class="hero" id="intro">
                <h1>Building a Chess Engine<br>From Random to AlphaZero</h1>
                <p>A comprehensive, hands-on tutorial for building chess AI from first principles</p>
                <p style="font-size: 0.9em; opacity: 0.8;">Risk-Reduction Philosophy ‚Ä¢ Baby Steps ‚Ä¢ Continuous Validation</p>
            </div>

            <!-- PROJECT VISION -->
            <section id="vision">
                <h2>üéØ Project Vision: Building AlphaZero from First Principles</h2>

                <p>This project teaches you how to build a <strong>chess AI that improves through self-play reinforcement learning</strong>, using the same fundamental techniques that powered DeepMind's AlphaZero‚Äîthe program that defeated Stockfish (the world's strongest chess engine) in 2017.</p>

                <p>But rather than jumping straight into the deep end with neural networks and self-play, we take an <strong>incremental, educational approach</strong> that builds your understanding step by step.</p>

                <h3>What Makes This Approach Different?</h3>

                <div class="comparison">
                    <div class="comparison-item">
                        <h4>‚ùå Traditional Approach</h4>
                        <ul>
                            <li>Start with deep learning immediately</li>
                            <li>Requires cloud GPUs from day 1</li>
                            <li>Nothing works until everything works</li>
                            <li>High risk of getting stuck</li>
                            <li>Hard to debug when things fail</li>
                        </ul>
                    </div>
                    <div class="comparison-item">
                        <h4>‚úÖ Our Risk-Reduction Approach</h4>
                        <ul>
                            <li>Build progressively sophisticated engines</li>
                            <li>Each phase delivers a playable program</li>
                            <li>Works on local hardware (Mac/PC)</li>
                            <li>Clear validation criteria at each step</li>
                            <li>Can stop at any phase and have something useful</li>
                        </ul>
                    </div>
                </div>

                <div class="key-insight">
                    <strong>üí° The Core Philosophy:</strong> Every phase in this tutorial produces a <strong>working, playable chess engine</strong>. You're never more than one phase away from something you can test, play against, and validate. This de-risks the learning process and ensures continuous progress.
                </div>

                <h3>The Ultimate Goal: AlphaZero-Style Self-Learning AI</h3>

                <p>By the end of this tutorial, you'll have built an AI that:</p>

                <ul class="checklist">
                    <li><strong>Learns from self-play:</strong> Improves by playing against itself (no human games needed)</li>
                    <li><strong>Combines search and learning:</strong> MCTS guided by neural network predictions</li>
                    <li><strong>Discovers novel strategies:</strong> Finds moves and patterns humans never taught it</li>
                    <li><strong>Scales with compute:</strong> Larger networks + more games = stronger play</li>
                    <li><strong>Generalizes:</strong> The techniques work for Go, shogi, and other board games</li>
                </ul>

                <div class="warning">
                    <strong>‚è±Ô∏è Time and Hardware Expectations:</strong>
                    <ul>
                        <li><strong>Phases 0-2 (Classical AI):</strong> 2-3 weeks, runs on any laptop (no GPU needed)</li>
                        <li><strong>Phase 3 (Neural Networks):</strong> 2-4 weeks, benefits from GPU (Mac M-series or NVIDIA)</li>
                        <li><strong>Phase 4 (Self-Play RL):</strong> 2-4 weeks, GPU highly recommended (can use cloud bursts)</li>
                        <li><strong>Phase 5 (Scaling):</strong> Ongoing, cloud GPU for training (2-6 hour bursts, ~$10-50/month)</li>
                    </ul>

                    <p><strong>Total time to AlphaZero-style AI:</strong> 8-14 weeks of part-time work (evenings/weekends)</p>
                </div>

                <h3>What You'll Learn Along the Way</h3>

                <p>This isn't just about chess‚Äîyou'll master fundamental AI concepts that apply across domains:</p>

                <table>
                    <tr>
                        <th>Phase</th>
                        <th>AI Concepts</th>
                        <th>Practical Skills</th>
                    </tr>
                    <tr>
                        <td>Phases 0-1</td>
                        <td>Game trees, minimax, alpha-beta pruning, evaluation functions</td>
                        <td>Classical search algorithms, heuristic design</td>
                    </tr>
                    <tr>
                        <td>Phase 2</td>
                        <td>Monte Carlo methods, UCT, exploration vs exploitation</td>
                        <td>Statistical sampling, anytime algorithms</td>
                    </tr>
                    <tr>
                        <td>Phase 3</td>
                        <td>Deep learning, ResNets, supervised learning, policy-value networks</td>
                        <td>PyTorch, neural network training, debugging ML models</td>
                    </tr>
                    <tr>
                        <td>Phases 4-5</td>
                        <td>Reinforcement learning, self-play, curriculum learning</td>
                        <td>Training loops, Elo systems, distributed computing</td>
                    </tr>
                </table>

                <div class="success">
                    <strong>üéì Prerequisites:</strong> You should be comfortable with Python programming and basic algorithms. We'll teach you chess AI, machine learning, and reinforcement learning from scratch‚Äîno prior experience required!
                </div>
            </section>

            <!-- ROADMAP & TIMELINE -->
            <section id="journey">
                <h2>üó∫Ô∏è The Complete Roadmap</h2>

                <div class="learning-objective">
                    <h3>Six Progressively Sophisticated Engines</h3>
                    <p>This tutorial guides you through building <strong>six chess engines</strong>, each playable and independently useful, culminating in an AlphaZero-style self-learning system.</p>

                    <p><strong>The Complete Journey:</strong></p>
                    <ul class="checklist">
                        <li><strong>Phase 0:</strong> Random Player (baseline, 1 hour)</li>
                        <li><strong>Phase 1:</strong> Minimax with hand-crafted evaluation (~1200-1400 Elo, 1 week)</li>
                        <li><strong>Phase 2:</strong> Monte Carlo Tree Search (~1400-1600 Elo, 1 week)</li>
                        <li><strong>Phase 3:</strong> Neural Network evaluation (~1400-1600 Elo, 2 weeks)</li>
                        <li><strong>Phase 4:</strong> Self-Play Reinforcement Learning (1600-1800+ Elo, 2-4 weeks)</li>
                        <li><strong>Phase 5:</strong> Scale up with larger networks and more compute (1800+ Elo, ongoing)</li>
                    </ul>
                </div>

                <div class="progress-bar">
                    <div class="progress-label">Overall Progress: 60% Complete (Phases 0-3 Done)</div>
                    <div class="progress-fill" style="width: 60%;"></div>
                </div>

                <h3>üéØ Why This Approach?</h3>

                <div class="key-insight">
                    <strong>üí° The Risk-Reduction Philosophy:</strong> Instead of jumping straight to deep learning and self-play, we build a <em>progression</em> of engines. Each phase:
                    <ul>
                        <li><strong>Delivers a playable chess program</strong> (you can always stop here)</li>
                        <li><strong>Teaches fundamental concepts</strong> needed for the next phase</li>
                        <li><strong>Has clear validation criteria</strong> (know when you're on track)</li>
                        <li><strong>Builds on previous work</strong> (nothing is thrown away)</li>
                        <li><strong>Works on consumer hardware</strong> (no cloud costs until you choose to scale)</li>
                    </ul>
                </div>

                <div class="warning">
                    <strong>‚ö†Ô∏è Prerequisites:</strong>
                    <ul>
                        <li>Python programming (intermediate level)</li>
                        <li>Basic understanding of algorithms and data structures</li>
                        <li>Familiarity with chess rules (not an expert, just know how pieces move)</li>
                        <li>Willingness to learn machine learning concepts (we'll teach from scratch)</li>
                    </ul>
                </div>

                <h3>üìö How to Use This Tutorial</h3>

                <p>This is designed as both a <strong>linear guide</strong> (read front-to-back) and a <strong>reference manual</strong> (jump to specific sections):</p>

                <ul>
                    <li><strong>First-time learners:</strong> Read sequentially, Phase 0 ‚Üí Phase 5</li>
                    <li><strong>Returning students:</strong> Use the sidebar to jump to specific phases</li>
                    <li><strong>Reference lookup:</strong> Check the glossary or search (Ctrl+F)</li>
                    <li><strong>Code review:</strong> All code is inline with detailed explanations</li>
                </ul>

                <div class="success">
                    <h4>‚úÖ By the End of This Tutorial, You Will:</h4>
                    <ul class="checklist">
                        <li>Understand game tree search (minimax, MCTS)</li>
                        <li>Build hand-crafted evaluation functions</li>
                        <li>Implement neural networks from scratch (policy-value architecture)</li>
                        <li>Understand reinforcement learning through self-play</li>
                        <li>Know how AlphaZero achieved superhuman chess strength</li>
                        <li>Have 6 working chess engines of increasing sophistication</li>
                        <li>Be able to extend and scale the system further</li>
                    </ul>
                </div>
            </section>

            <!-- PHASE 0: RANDOM PLAYER -->
            <details class="phase" id="phase0" open>
                <summary>üìç Phase 0: Random Player Foundation</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 0: Random Player
                    </div>

                    <div class="day-marker complete">‚úÖ Foundation - 1 Hour</div>

                    <div class="learning-objective">
                        <h3>Learning Objectives - Phase 0</h3>
                        <p>By the end of this phase, you will:</p>
                        <ul class="checklist">
                            <li>Set up the development environment</li>
                            <li>Understand the python-chess library</li>
                            <li>Build a complete game loop (random vs random)</li>
                            <li>Implement move validation and game termination</li>
                            <li>Create a playable baseline for comparison</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: The Simplest Possible Chess Program</h3>

                    <p>Before building sophisticated AI, we need a <strong>working baseline</strong>. This phase teaches:</p>
                    <ul>
                        <li>How to represent chess boards and moves in code</li>
                        <li>How to check for game termination (checkmate, stalemate, draws)</li>
                        <li>How to build a complete game loop</li>
                    </ul>

                    <div class="key-insight">
                        <strong>üí° Why Start with Random?</strong> The random player establishes a <em>baseline</em> for measuring improvement. Every future engine must beat random play by a large margin. It also validates that your chess infrastructure (move generation, game rules) works correctly before adding complexity.
                    </div>

                    <h3>üõ†Ô∏è Setup: Install Dependencies</h3>

                    <p>We'll use the <code>python-chess</code> library, which handles all the complex chess rules:</p>

                    <pre><code class="language-bash"># Create project directory
mkdir Chess_RL
cd Chess_RL

# Create virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install python-chess
pip install python-chess>=1.999</code></pre>

                    <div class="code-explanation">
                        <strong>üîç Why python-chess?</strong>
                        <ul>
                            <li>Handles all chess rules (castling, en passant, etc.)</li>
                            <li>Move generation is fast and bug-free</li>
                            <li>Saves 2-4 weeks of implementing bitboards from scratch</li>
                            <li>Battle-tested in thousands of chess programs</li>
                        </ul>
                    </div>

                    <h3>üíª Code: Random vs Random Game</h3>

                    <p>Create <code>cli/play.py</code>:</p>

                    <pre><code class="language-python">#!/usr/bin/env python3
"""
Phase 0: Random Player - The Simplest Chess Program

This demonstrates:
1. Complete game loop
2. Random move selection
3. Game termination detection
4. Result reporting
"""

import chess
import random


def random_move(board):
    """
    Select a random legal move from the current position.

    Args:
        board: chess.Board object

    Returns:
        chess.Move object (randomly selected)
    """
    legal_moves = list(board.legal_moves)
    if not legal_moves:
        return None
    return random.choice(legal_moves)


def play_random_vs_random():
    """Play a complete game: Random vs Random."""
    # Create starting position
    board = chess.Board()
    move_count = 0

    print("üé≤ Random vs Random Chess Game")
    print("Starting position:")
    print(board)
    print()

    # Game loop
    while not board.is_game_over():
        # Select and make move
        move = random_move(board)
        board.push(move)
        move_count += 1

        # Display current position
        print(f"Move {move_count}: {move}")
        print(board)
        print()

    # Game over - report result
    result = board.result()
    outcome = board.outcome()

    print("=" * 50)
    print(f"üèÅ GAME OVER after {move_count} moves")
    print("=" * 50)
    print(f"Result: {result}")
    print(f"Termination: {outcome.termination.name}")

    if outcome.winner is None:
        print("Draw!")
    elif outcome.winner == chess.WHITE:
        print("White wins!")
    else:
        print("Black wins!")


if __name__ == "__main__":
    play_random_vs_random()
</code></pre>

                    <div class="code-explanation">
                        <strong>üîç Code Walkthrough:</strong>
                        <ul>
                            <li><code>chess.Board()</code>: Creates starting position</li>
                            <li><code>board.legal_moves</code>: All legal moves in current position</li>
                            <li><code>board.push(move)</code>: Makes a move on the board</li>
                            <li><code>board.is_game_over()</code>: Checks for checkmate/stalemate/draw</li>
                            <li><code>board.result()</code>: Returns "1-0" (White wins), "0-1" (Black wins), or "1/2-1/2" (draw)</li>
                        </ul>
                    </div>

                    <h3>üß™ Testing: Run Your First Game</h3>

                    <pre><code class="language-bash">python cli/play.py</code></pre>

                    <p>You should see a complete chess game play out with random moves!</p>

                    <h3>üìä What to Observe</h3>

                    <div class="diagram">
<strong>Typical Random Game Characteristics:</strong>
<pre>
Game Length:      40-100 moves (random is inefficient)
Termination:      Usually checkmate (random stumbles into it)
Piece Trades:     Frequent (no strategy, just chaos)
Checkmates:       Often take many moves to deliver
Draw Rate:        Low (~10-20%, random rarely achieves draws)
</pre>
                    </div>

                    <div class="checkpoint">
                        <h4>‚úÖ Phase 0 Validation Checklist</h4>
                        <p>Before moving to Phase 1, verify:</p>
                        <ul class="checklist">
                            <li>Game completes without errors</li>
                            <li>All games terminate (checkmate, stalemate, or draw)</li>
                            <li>Move count is reasonable (20-150 moves)</li>
                            <li>Both colors can win</li>
                            <li>Draws are detected correctly</li>
                        </ul>

                        <p><strong>Test:</strong> Run 10 games and record results. You should see a mix of White wins, Black wins, and occasional draws.</p>
                    </div>

                    <h3>üéì Key Concepts Learned</h3>

                    <table>
                        <tr>
                            <th>Concept</th>
                            <th>What You Learned</th>
                        </tr>
                        <tr>
                            <td>Game Loop</td>
                            <td>Initialize ‚Üí Select Move ‚Üí Make Move ‚Üí Check Termination ‚Üí Repeat</td>
                        </tr>
                        <tr>
                            <td>Move Generation</td>
                            <td>python-chess generates all legal moves automatically</td>
                        </tr>
                        <tr>
                            <td>Game Termination</td>
                            <td>Checkmate, stalemate, 50-move rule, threefold repetition</td>
                        </tr>
                        <tr>
                            <td>Random Baseline</td>
                            <td>Establishes 0% skill - all future engines must beat this</td>
                        </tr>
                    </table>

                    <div class="success">
                        <h4>üéâ Phase 0 Complete!</h4>
                        <p>You've built a working chess program! It's not smart, but it's a solid foundation.</p>
                        <p><strong>What's next:</strong> In Phase 1, we'll add <em>evaluation</em> (understanding which positions are good) and <em>search</em> (looking ahead to find forcing moves).</p>
                    </div>

                    <div class="section-nav">
                        <a href="#intro" class="prev">‚Üê Introduction</a>
                        <a href="#phase1" class="next">Next: Minimax & Evaluation ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- PHASE 1: MINIMAX & EVALUATION -->
            <details class="phase" id="phase1">
                <summary>üìç Phase 1: Minimax & Evaluation Functions</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 1: Minimax & Evaluation
                    </div>

                    <div class="day-marker complete">‚úÖ Week 1 - Target: 1200-1400 Elo</div>

                    <div class="learning-objective">
                        <h3>Learning Objectives - Phase 1</h3>
                        <p>By the end of this phase, you will:</p>
                        <ul class="checklist">
                            <li>Implement material evaluation (piece counting)</li>
                            <li>Build minimax search with alpha-beta pruning</li>
                            <li>Add positional evaluation (center control, development)</li>
                            <li>Implement strategic evaluation (king safety, pawn structure)</li>
                            <li>Add quiescence search to eliminate horizon effect</li>
                            <li>Create UCI interface for playing in chess GUIs</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: From Random to Strategic Play</h3>

                    <p>Phase 1 transforms our random player into a chess engine that understands <strong>evaluation</strong> (what makes a position good) and <strong>search</strong> (looking ahead to find forcing moves).</p>

                    <div class="key-insight">
                        <strong>üí° The Two Pillars of Chess AI:</strong>
                        <ul>
                            <li><strong>Evaluation:</strong> Given a position, how good is it? (+200 = White is winning by 2 pawns)</li>
                            <li><strong>Search:</strong> Look ahead N moves to see which position we'll reach</li>
                        </ul>
                        Combined, these create an engine that can find tactics and avoid blunders.
                    </div>

                    <h3>Day 2: Material Evaluation</h3>

                    <p>The first rule of chess: <strong>don't lose your pieces</strong>. We'll teach our engine the point value of pieces:</p>

                    <pre><code>def evaluate_material(board: chess.Board) -> int:
    """Evaluate material balance. Returns score in centipawns."""
    PIECE_VALUES = {
        chess.PAWN: 100,
        chess.KNIGHT: 300,
        chess.BISHOP: 300,
        chess.ROOK: 500,
        chess.QUEEN: 900,
        chess.KING: 0,  # Priceless
    }

    score = 0
    for square in chess.SQUARES:
        piece = board.piece_at(square)
        if piece:
            value = PIECE_VALUES[piece.piece_type]
            score += value if piece.color == chess.WHITE else -value

    return score  # Positive = White winning, Negative = Black winning</code></pre>

                    <div class="success">
                        <strong>‚úÖ Test:</strong> Starting position returns 0 (equal material). Position missing White queen returns -900.
                    </div>

                    <h3>Days 3-4: Minimax Search with Alpha-Beta Pruning</h3>

                    <p>Now we add <strong>look-ahead</strong>. Minimax explores the game tree, assuming both players play optimally:</p>

                    <div class="diagram">
<pre>
                [Current Position]
                    Your turn
                        |
    +-------------------+-------------------+
    |                   |                   |
Move A              Move B              Move C
    |                   |                   |
[After A]          [After B]          [After C]
 Opp turn           Opp turn           Opp turn
    |                   |                   |
Pick best       Pick best           Pick best
response        response            response

White searches each move, assumes Black responds
optimally, then picks the move with best result.</pre>
                    </div>

                    <pre><code>def minimax(board, depth, alpha, beta, maximizing):
    """Minimax with alpha-beta pruning."""
    if depth == 0 or board.is_game_over():
        return evaluate(board)

    legal_moves = list(board.legal_moves)
    ordered_moves = order_moves(board, legal_moves)  # Search good moves first

    if maximizing:
        max_eval = float('-inf')
        for move in ordered_moves:
            board.push(move)
            eval_score = minimax(board, depth - 1, alpha, beta, False)
            board.pop()

            max_eval = max(max_eval, eval_score)
            alpha = max(alpha, eval_score)

            if beta <= alpha:
                break  # Beta cutoff - prune!

        return max_eval
    else:
        min_eval = float('inf')
        for move in ordered_moves:
            board.push(move)
            eval_score = minimax(board, depth - 1, alpha, beta, True)
            board.pop()

            min_eval = min(min_eval, eval_score)
            beta = min(beta, eval_score)

            if beta <= alpha:
                break  # Alpha cutoff - prune!

        return min_eval</code></pre>

                    <div class="key-insight">
                        <strong>Alpha-Beta Magic:</strong> With good move ordering, alpha-beta pruning reduces nodes searched from ~1.5M to ~40K at depth 4 (40√ó speedup!)
                    </div>

                    <h3>Day 5: Positional Understanding</h3>

                    <p>Material alone isn't enough. We add <strong>center control</strong> and <strong>piece development</strong>:</p>

                    <pre><code>def evaluate_center_control(board):
    """Reward pieces in the center (d4, e4, d5, e5)."""
    CENTER = [chess.D4, chess.E4, chess.D5, chess.E5]
    score = 0

    for square in CENTER:
        piece = board.piece_at(square)
        if piece:
            bonus = 30 if piece.piece_type == chess.PAWN else 20
            score += bonus if piece.color == chess.WHITE else -bonus

    return score

def evaluate_development(board):
    """Penalize knights/bishops on starting squares (opening only)."""
    if board.fullmove_number > 10:
        return 0  # Only care about development in opening

    WHITE_KNIGHT_START = [chess.B1, chess.G1]
    score = 0

    for square in WHITE_KNIGHT_START:
        piece = board.piece_at(square)
        if piece and piece.piece_type == chess.KNIGHT:
            score -= 15  # Penalty for undeveloped piece

    # (Similar logic for Black knights and bishops)
    return score</code></pre>

                    <h3>Day 6: Strategic Evaluation</h3>

                    <p>Add <strong>king safety</strong> and <strong>pawn structure</strong>:</p>

                    <pre><code>def evaluate_king_safety(board):
    """Reward castling and pawn shields."""
    score = 0

    white_king_sq = board.king(chess.WHITE)

    # Castled kingside
    if white_king_sq == chess.G1:
        score += 30  # Castling bonus

        # Pawn shield bonus
        if board.piece_at(chess.F2) == chess.Piece(chess.PAWN, chess.WHITE):
            score += 10
        if board.piece_at(chess.G2) == chess.Piece(chess.PAWN, chess.WHITE):
            score += 10

    # Penalty for king in center (dangerous!)
    if white_king_sq in [chess.D4, chess.E4, chess.D5, chess.E5]:
        score -= 40

    # (Similar logic for Black king)
    return score

def evaluate_pawn_structure(board):
    """Penalize doubled/isolated pawns, reward passed pawns."""
    score = 0

    for file in range(8):
        white_pawns = [r for r in range(8)
                       if board.piece_at(chess.square(file, r)) ==
                          chess.Piece(chess.PAWN, chess.WHITE)]

        # Doubled pawns penalty
        if len(white_pawns) > 1:
            score -= 20 * (len(white_pawns) - 1)

        # Passed pawns bonus (no enemy pawns blocking)
        for rank in white_pawns:
            if is_passed_pawn(board, file, rank, chess.WHITE):
                score += 20 + (rank * 10)  # More valuable as it advances

    # (Similar logic for Black pawns)
    return score</code></pre>

                    <h3>Complete Evaluation Function</h3>

                    <pre><code>def evaluate(board):
    """Master evaluation combining all components."""
    if board.is_checkmate():
        return -100000 if board.turn == chess.WHITE else 100000

    if board.is_stalemate() or board.is_insufficient_material():
        return 0  # Draw

    # Material dominates (~80% of evaluation)
    score = evaluate_material(board)

    # Positional factors (~10-50 centipawns each)
    score += evaluate_center_control(board)
    score += evaluate_development(board)
    score += evaluate_king_safety(board)
    score += evaluate_pawn_structure(board)

    # Small bonuses
    if board.is_check():
        score += 50 if board.turn == chess.BLACK else -50

    mobility = board.legal_moves.count()
    score += mobility if board.turn == chess.WHITE else -mobility

    return score</code></pre>

                    <div class="key-insight">
                        <strong>Evaluation Hierarchy:</strong> Material = 100-900 points per piece. Positional = 10-50 points. This balance ensures material is most important, but positional factors decide equal positions.
                    </div>

                    <h3>Day 7: Quiescence Search - Fixing the Horizon Effect</h3>

                    <p>Critical fix: <strong>don't stop searching in the middle of tactics</strong>!</p>

                    <div class="warning">
                        <strong>‚ö†Ô∏è The Horizon Effect:</strong> Fixed-depth search stops at depth N, even if a tactical blow comes at depth N+1. The engine thinks it won a pawn, but doesn't see the piece gets recaptured next move!
                    </div>

                    <p><strong>Solution:</strong> Continue searching captures/checks until position is "quiet" (no immediate tactics).</p>

                    <pre><code>def quiescence_search(board, alpha, beta, maximizing, max_depth=10):
    """Search tactical moves until position is quiet."""
    if board.is_game_over() or max_depth <= 0:
        return evaluate(board)

    # Stand-pat: evaluation if we make no more captures
    stand_pat = evaluate(board)

    if maximizing:
        if stand_pat >= beta:
            return beta
        alpha = max(alpha, stand_pat)
    else:
        if stand_pat <= alpha:
            return alpha
        beta = min(beta, stand_pat)

    # Only search captures, checks, promotions (tactical moves)
    noisy_moves = [m for m in board.legal_moves
                   if board.is_capture(m) or m.promotion]

    if not noisy_moves:
        return stand_pat  # Position is quiet!

    # Search noisy moves recursively
    if maximizing:
        for move in order_moves(board, noisy_moves):
            board.push(move)
            score = quiescence_search(board, alpha, beta, False, max_depth - 1)
            board.pop()

            if score >= beta:
                return beta
            alpha = max(alpha, score)
        return alpha
    else:
        # (Similar logic for minimizing)
        ...

# Integrate into minimax:
def minimax(board, depth, alpha, beta, maximizing):
    if depth == 0:
        return quiescence_search(board, alpha, beta, maximizing)  # ‚úÖ Fixed!
    # (rest of minimax unchanged)</code></pre>

                    <div class="success">
                        <strong>‚úÖ Impact:</strong> Quiescence search eliminates tactical blunders. Expected Elo gain: +100 to +200!
                    </div>

                    <h3>Day 8: UCI Protocol Integration</h3>

                    <p>Make your engine playable in chess GUIs (Arena, Cute Chess, etc.) via the Universal Chess Interface:</p>

                    <pre><code>class UCIEngine:
    """UCI-compliant chess engine wrapper."""

    def __init__(self):
        self.board = chess.Board()
        self.search_depth = 3

    def handle_uci(self):
        print("id name Chess_RL v0.1.0")
        print("id author Your Name")
        print("option name Search Depth type spin default 3 min 1 max 6")
        print("uciok", flush=True)

    def handle_position(self, args):
        if args[0] == "startpos":
            self.board = chess.Board()
            idx = 1
        # Apply moves if present
        if "moves" in args:
            move_idx = args.index("moves") + 1
            for move_str in args[move_idx:]:
                self.board.push(chess.Move.from_uci(move_str))

    def handle_go(self, args):
        depth = self.search_depth
        if "depth" in args:
            depth = int(args[args.index("depth") + 1])

        best_move = best_move_minimax(self.board, depth)
        print(f"bestmove {best_move.uci()}", flush=True)

    def run(self):
        while True:
            line = input().strip()
            if not line:
                continue

            parts = line.split()
            command = parts[0]

            if command == "uci":
                self.handle_uci()
            elif command == "isready":
                print("readyok", flush=True)
            elif command == "position":
                self.handle_position(parts[1:])
            elif command == "go":
                self.handle_go(parts[1:])
            elif command == "quit":
                break</code></pre>

                    <div class="checkpoint">
                        <h4>‚úÖ Phase 1 Validation Checklist</h4>
                        <p>Before moving to Phase 2, verify:</p>
                        <ul class="checklist">
                            <li>Engine finds mate-in-2 puzzles</li>
                            <li>Avoids hanging pieces (no horizon effect blunders)</li>
                            <li>Develops pieces in opening (plays e4/d4/Nf3, not h4)</li>
                            <li>Beats random player 100% of the time</li>
                            <li>UCI interface works in a chess GUI</li>
                            <li>Estimated strength: 1200-1400 Elo</li>
                        </ul>
                    </div>

                    <div class="success">
                        <h4>üéâ Phase 1 Complete!</h4>
                        <p>You've built a chess engine with human-like understanding! It evaluates positions, searches ahead, finds tactics, and plays strategically.</p>
                        <p><strong>What's next:</strong> Phase 2 introduces Monte Carlo Tree Search (MCTS), a different search algorithm that uses statistical sampling instead of exhaustive search.</p>
                    </div>

                    <div class="section-nav">
                        <a href="#phase0" class="prev">‚Üê Phase 0: Random Player</a>
                        <a href="#phase2" class="next">Next: Monte Carlo Tree Search ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- PHASE 2: MONTE CARLO TREE SEARCH -->
            <details class="phase" id="phase2">
                <summary>üìç Phase 2: Monte Carlo Tree Search (MCTS)</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 2: Monte Carlo Tree Search
                    </div>

                    <div class="day-marker complete">‚úÖ Week 2 - Target: 1400-1600 Elo</div>

                    <div class="learning-objective">
                        <h3>Learning Objectives - Phase 2</h3>
                        <p>By the end of this phase, you will:</p>
                        <ul class="checklist">
                            <li>Understand Monte Carlo Tree Search fundamentals</li>
                            <li>Implement UCT (Upper Confidence Bound for Trees) selection</li>
                            <li>Build MCTS with random rollouts</li>
                            <li>Enhance MCTS with domain knowledge (evaluation function)</li>
                            <li>Compare MCTS vs Minimax performance</li>
                            <li>Understand when MCTS excels vs traditional search</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: A Different Way to Search</h3>

                    <p>Minimax exhaustively searches all moves to a fixed depth. MCTS takes a different approach: <strong>statistically sample the most promising paths</strong> by playing out random games.</p>

                    <div class="key-insight">
                        <strong>üí° The MCTS Philosophy:</strong>
                        <ul>
                            <li><strong>Minimax:</strong> "Explore everything to depth N"</li>
                            <li><strong>MCTS:</strong> "Focus computational budget on promising moves through statistical sampling"</li>
                        </ul>
                        MCTS can search deeper on critical variations while spending less time on obviously bad moves.
                    </div>

                    <h3>How MCTS Works: The Four Phases</h3>

                    <div class="diagram">
<pre>
MCTS operates in a loop, repeating these 4 steps N times:

1. SELECTION: Start at root, follow promising path down tree
   ‚Üì
2. EXPANSION: Add a new node (unexplored move)
   ‚Üì
3. SIMULATION: Play out a random game to the end
   ‚Üì
4. BACKPROPAGATION: Update statistics along path

Repeat N times (e.g., 200-800 simulations)
Then pick the move with most visits</pre>
                    </div>

                    <h3>Phase 1: Selection with UCT</h3>

                    <p>At each node, we select the child with the highest <strong>UCT score</strong>:</p>

                    <div class="key-insight">
                        <strong>UCT Formula:</strong> UCT = (wins/visits) + C √ó ‚àö(ln(parent_visits) / visits)
                        <ul>
                            <li><strong>Exploitation term:</strong> wins/visits (favor moves that win often)</li>
                            <li><strong>Exploration term:</strong> C √ó ‚àö(...) (try less-visited moves)</li>
                            <li><strong>C constant:</strong> Balance exploration vs exploitation (typically ‚àö2)</li>
                        </ul>
                    </div>

                    <pre><code>class MCTSNode:
    def __init__(self, board, parent=None, move=None):
        self.board = board.copy()
        self.parent = parent
        self.move = move  # Move that led to this node

        self.children = []
        self.visits = 0
        self.wins = 0.0  # From perspective of player who moved here

        self.untried_moves = list(board.legal_moves)

    def uct_value(self, exploration_constant=1.41):  # ‚àö2
        """Calculate UCT score for this node."""
        if self.visits == 0:
            return float('inf')  # Unvisited nodes have highest priority

        # Exploitation: win rate
        exploitation = self.wins / self.visits

        # Exploration: ‚àö(ln(parent_visits) / visits)
        exploration = exploration_constant * math.sqrt(
            math.log(self.parent.visits) / self.visits
        )

        return exploitation + exploration

    def select_child(self):
        """Select child with highest UCT value."""
        return max(self.children, key=lambda c: c.uct_value())</code></pre>

                    <h3>Phase 2: Expansion</h3>

                    <p>When we reach a node with untried moves, we expand it:</p>

                    <pre><code>    def expand(self):
        """Add a new child node for an untried move."""
        if not self.untried_moves:
            return None  # Fully expanded

        # Pick a random untried move
        move = self.untried_moves.pop()

        # Create new board state
        new_board = self.board.copy()
        new_board.push(move)

        # Create child node
        child = MCTSNode(new_board, parent=self, move=move)
        self.children.append(child)

        return child</code></pre>

                    <h3>Phase 3: Simulation (Random Rollout)</h3>

                    <p>Play out a random game from this position to estimate value:</p>

                    <pre><code>def simulate_random_game(board):
    """Play random moves until game ends. Return result."""
    sim_board = board.copy()

    while not sim_board.is_game_over():
        legal_moves = list(sim_board.legal_moves)
        random_move = random.choice(legal_moves)
        sim_board.push(random_move)

    # Return game result from White's perspective
    result = sim_board.result()
    if result == "1-0":
        return 1.0  # White wins
    elif result == "0-1":
        return 0.0  # Black wins
    else:
        return 0.5  # Draw</code></pre>

                    <div class="warning">
                        <strong>‚ö†Ô∏è Random Rollouts are Weak:</strong> Pure random games give noisy estimates. We'll improve this with domain knowledge later!
                    </div>

                    <h3>Phase 4: Backpropagation</h3>

                    <p>Update statistics along the path from leaf to root:</p>

                    <pre><code>    def backpropagate(self, result):
        """Update node statistics with simulation result."""
        self.visits += 1

        # Result is from White's perspective
        # If this node is Black's turn, we want opposite result
        if self.board.turn == chess.BLACK:
            self.wins += result
        else:
            self.wins += (1.0 - result)

        # Recurse to parent
        if self.parent:
            self.parent.backpropagate(result)</code></pre>

                    <h3>Putting It All Together: MCTS Main Loop</h3>

                    <pre><code>def mcts_search(board, num_simulations=200):
    """Run MCTS for given number of simulations."""
    root = MCTSNode(board)

    for _ in range(num_simulations):
        node = root

        # 1. SELECTION: Traverse tree using UCT
        while node.untried_moves == [] and node.children:
            node = node.select_child()

        # 2. EXPANSION: Add new node if possible
        if node.untried_moves:
            node = node.expand()

        # 3. SIMULATION: Play random game from this node
        result = simulate_random_game(node.board)

        # 4. BACKPROPAGATION: Update statistics
        node.backpropagate(result)

    # Choose move with most visits (most robust)
    best_child = max(root.children, key=lambda c: c.visits)
    return best_child.move</code></pre>

                    <h3>Improving MCTS: Domain Knowledge</h3>

                    <p>Pure random rollouts are weak. We can improve by using our evaluation function:</p>

                    <pre><code>def simulate_with_evaluation(board, max_depth=10):
    """Simulate game using evaluation function instead of random play."""
    sim_board = board.copy()
    depth = 0

    while not sim_board.is_game_over() and depth < max_depth:
        legal_moves = list(sim_board.legal_moves)

        # Instead of random, pick move with best evaluation
        best_move = None
        best_eval = float('-inf') if sim_board.turn == chess.WHITE else float('inf')

        for move in legal_moves:
            sim_board.push(move)
            eval_score = evaluate(sim_board)  # Use our evaluation!
            sim_board.pop()

            if sim_board.turn == chess.WHITE:
                if eval_score > best_eval:
                    best_eval = eval_score
                    best_move = move
            else:
                if eval_score < best_eval:
                    best_eval = eval_score
                    best_move = move

        if best_move:
            sim_board.push(best_move)
            depth += 1

    # Return evaluation of final position (normalized to [0, 1])
    final_eval = evaluate(sim_board)
    return 1.0 / (1.0 + math.exp(-final_eval / 100))  # Sigmoid normalization</code></pre>

                    <div class="success">
                        <strong>‚úÖ Impact:</strong> Using evaluation function in rollouts dramatically improves MCTS strength! Now it understands tactics and strategy, not just random moves.
                    </div>

                    <h3>MCTS vs Minimax: When to Use Each?</h3>

                    <table>
                        <tr>
                            <th>Characteristic</th>
                            <th>Minimax + Alpha-Beta</th>
                            <th>MCTS</th>
                        </tr>
                        <tr>
                            <td><strong>Search Style</strong></td>
                            <td>Exhaustive to fixed depth</td>
                            <td>Adaptive (deep on promising moves)</td>
                        </tr>
                        <tr>
                            <td><strong>Time Management</strong></td>
                            <td>Depth limited</td>
                            <td>Simulation limited (anytime algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>Tactical Positions</strong></td>
                            <td>‚úÖ Excellent with quiescence</td>
                            <td>‚ö†Ô∏è Can miss deep tactics</td>
                        </tr>
                        <tr>
                            <td><strong>Complex Positions</strong></td>
                            <td>‚ö†Ô∏è Struggles with high branching</td>
                            <td>‚úÖ Adapts to complexity</td>
                        </tr>
                        <tr>
                            <td><strong>Neural Network Ready</strong></td>
                            <td>‚ùå Hard to integrate learning</td>
                            <td>‚úÖ Perfect for AlphaZero-style RL</td>
                        </tr>
                    </table>

                    <div class="key-insight">
                        <strong>Why MCTS for Phase 3?</strong> MCTS is the foundation of AlphaZero! In Phase 3, we'll replace random/evaluation rollouts with a neural network that provides policy (which moves to explore) and value (position evaluation). MCTS + Neural Network = AlphaZero magic!
                    </div>

                    <h3>Performance Tuning: Key Parameters</h3>

                    <ul>
                        <li><strong>Number of simulations:</strong> 200-800 for reasonable play (more = stronger but slower)</li>
                        <li><strong>Exploration constant (C):</strong> ‚àö2 (1.41) is standard, increase for more exploration</li>
                        <li><strong>Rollout depth:</strong> 10-20 moves (balance between speed and accuracy)</li>
                        <li><strong>Evaluation vs random:</strong> Evaluation-based rollouts are much stronger</li>
                    </ul>

                    <div class="checkpoint">
                        <h4>‚úÖ Phase 2 Validation Checklist</h4>
                        <p>Before moving to Phase 3, verify:</p>
                        <ul class="checklist">
                            <li>MCTS with 200 sims beats random player 100% of the time</li>
                            <li>MCTS with evaluation rollouts matches/beats minimax depth-3</li>
                            <li>UCT properly balances exploration and exploitation</li>
                            <li>Engine doesn't hang (simulations complete in reasonable time)</li>
                            <li>Move selection is robust (high visit count moves)</li>
                            <li>Estimated strength: 1400-1600 Elo</li>
                        </ul>
                    </div>

                    <div class="success">
                        <h4>üéâ Phase 2 Complete!</h4>
                        <p>You've implemented Monte Carlo Tree Search, a fundamentally different search algorithm from minimax. This prepares you for Phase 3, where MCTS becomes the bridge between search and deep learning!</p>
                        <p><strong>What's next:</strong> Phase 3 introduces neural networks that learn to evaluate positions and suggest moves, replacing hand-crafted evaluation functions with learned representations.</p>
                    </div>

                    <div class="section-nav">
                        <a href="#phase1" class="prev">‚Üê Phase 1: Minimax</a>
                        <a href="#phase3" class="next">Next: Neural Networks ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- PHASE 3: NEURAL NETWORKS (OVERVIEW + LINK) -->
            <details class="phase" id="phase3">
                <summary>üìç Phase 3: Neural Networks (Current Phase)</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 3: Neural Networks
                    </div>

                    <div class="day-marker current">üîÑ Weeks 3-4 - Target: 1400-1600 Elo</div>

                    <div class="learning-objective">
                        <h3>Learning Objectives - Phase 3</h3>
                        <p>By the end of this phase, you will:</p>
                        <ul class="checklist">
                            <li>Understand AlphaZero-style policy-value networks</li>
                            <li>Implement ResNet architecture for chess</li>
                            <li>Build board encoding (position ‚Üí tensor)</li>
                            <li>Implement move encoding/decoding (move ‚Üî integer)</li>
                            <li>Train neural network on supervised data (Lichess games)</li>
                            <li>Integrate neural network with MCTS</li>
                            <li>Compare NN-MCTS vs hand-crafted evaluation</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: From Hand-Crafted to Learned Evaluation</h3>

                    <p>In Phase 1, we hand-crafted evaluation functions (material, center control, king safety). In Phase 3, we replace these with a <strong>neural network that learns patterns from millions of chess games</strong>.</p>

                    <div class="key-insight">
                        <strong>üí° The Neural Network Advantage:</strong>
                        <ul>
                            <li><strong>Hand-crafted eval:</strong> Limited by human chess knowledge (hundreds of heuristics max)</li>
                            <li><strong>Neural network:</strong> Learns millions of patterns from data (can discover non-obvious strategies)</li>
                            <li><strong>Scalability:</strong> More data + bigger network = stronger play (no human bottleneck)</li>
                        </ul>
                    </div>

                    <h3>The AlphaZero Architecture: Policy + Value</h3>

                    <p>Unlike traditional engines with one evaluation function, AlphaZero uses a <strong>two-headed network</strong>:</p>

                    <div class="diagram">
<pre>
Input: Board Position (8√ó8√ó20 tensor)
  ‚Üì
Shared ResNet Trunk (4-40 residual blocks)
  ‚Üì
  ‚îú‚îÄ‚îÄ‚Üí Policy Head: P(move | position)
  ‚îÇ    Output: 4672-dim vector (probability for each move)
  ‚îÇ    Usage: Tells MCTS which moves to explore
  ‚îÇ
  ‚îî‚îÄ‚îÄ‚Üí Value Head: V(position)
       Output: Scalar in [-1, +1]
       Usage: Tells MCTS how good the position is
</pre>
                    </div>

                    <div class="key-insight">
                        <strong>Why Two Heads?</strong>
                        <ul>
                            <li><strong>Policy:</strong> "Which moves look promising?" (guides MCTS exploration)</li>
                            <li><strong>Value:</strong> "Is this position winning?" (replaces rollouts in MCTS)</li>
                        </ul>
                        Together, they make MCTS much more efficient: explore good moves (policy) and evaluate positions accurately (value).
                    </div>

                    <h3>Current Progress (Days 1-2 Complete)</h3>

                    <div class="success">
                        <h4>‚úÖ Implemented Components:</h4>
                        <ul class="checklist">
                            <li><strong>net/model.py:</strong> PolicyValueNetwork (ResNet with policy/value heads)</li>
                            <li><strong>net/encoding.py:</strong> board_to_tensor(), move_to_index(), legal_moves_mask()</li>
                            <li><strong>Network architecture:</strong> 4 ResBlocks, 128 channels, ~1.4M parameters</li>
                            <li><strong>Input encoding:</strong> 20-plane representation (pieces, castling, turn, etc.)</li>
                            <li><strong>Move encoding:</strong> 4672-dim action space (from_square √ó 64 + to_square)</li>
                        </ul>
                    </div>

                    <h3>Detailed Tutorial: Phase 3 Deep Dive</h3>

                    <p>Phase 3 is comprehensive and spans multiple weeks. We've created a dedicated <strong>53 KB standalone tutorial</strong> with extensive pedagogical content covering:</p>

                    <ul>
                        <li>Complete neural network architecture walkthrough</li>
                        <li>Detailed board encoding explanation with examples</li>
                        <li>Training pipeline (dataset, loss functions, optimization)</li>
                        <li>Integration with MCTS (replacing rollouts with NN evaluation)</li>
                        <li>Debugging neural networks for chess</li>
                        <li>Performance benchmarks and Elo comparisons</li>
                    </ul>

                    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 8px; text-align: center; margin: 30px 0;">
                        <h3 style="color: white; margin-top: 0;">üìò Complete Phase 3 Tutorial</h3>
                        <p style="color: white; font-size: 1.1em; margin: 20px 0;">
                            For the full neural network tutorial with code walkthroughs, diagrams, and pedagogical best practices:
                        </p>
                        <a href="neural_network_tutorial.html" style="display: inline-block; background: white; color: #667eea; padding: 15px 40px; border-radius: 25px; text-decoration: none; font-weight: bold; font-size: 1.2em; box-shadow: 0 4px 6px rgba(0,0,0,0.2); transition: transform 0.2s;">
                            Open Neural Network Tutorial ‚Üí
                        </a>
                        <p style="color: white; margin-top: 20px; opacity: 0.9; font-size: 0.95em;">
                            (~1,300 lines ‚Ä¢ ~12,000 words ‚Ä¢ Bloom's Taxonomy ‚Ä¢ Multiple representations)
                        </p>
                    </div>

                    <h3>Quick Reference: Key Concepts</h3>

                    <h4>Input Encoding (20 planes √ó 8√ó8)</h4>
                    <pre><code># Board state ‚Üí 20-plane tensor
Planes 0-11:  Piece positions (6 piece types √ó 2 colors)
Plane 12:     Repetition count = 1
Plane 13:     Repetition count = 2
Planes 14-17: Castling rights (WK, WQ, BK, BQ)
Plane 18:     No-progress count (50-move rule)
Plane 19:     Side to move (all 1s if White, all 0s if Black)</code></pre>

                    <h4>Network Architecture</h4>
                    <pre><code>PolicyValueNetwork(
  input_channels=20,
  num_res_blocks=4,    # Increase to 10-40 for stronger play
  num_channels=128     # Increase to 256 for more capacity
)
  ‚îú‚îÄ Shared trunk: Conv ‚Üí [ResBlock √ó 4] ‚Üí BatchNorm
  ‚îú‚îÄ Policy head: Conv ‚Üí Flatten ‚Üí Linear(4672)
  ‚îî‚îÄ Value head: Conv ‚Üí Flatten ‚Üí Linear ‚Üí Tanh()</code></pre>

                    <h4>Integration with MCTS</h4>
                    <pre><code># In MCTS simulation, replace random rollout with NN:
def simulate_with_neural_network(board):
    """Use neural network instead of hand-crafted evaluation."""
    tensor = board_to_tensor(board)
    policy, value = model.predict(tensor)

    # Value is already in [-1, +1] (White winning to Black winning)
    # Normalize to [0, 1] for MCTS backpropagation
    return (value + 1.0) / 2.0

# In MCTS expansion, use policy to guide selection:
def expand_with_policy(node):
    """Use policy network to prioritize moves."""
    tensor = board_to_tensor(node.board)
    policy, _ = model.predict(tensor)

    # Sort untried moves by policy probability
    move_probs = [(move, policy[move_to_index(move)])
                  for move in node.untried_moves]
    move_probs.sort(key=lambda x: x[1], reverse=True)

    # Expand highest-probability move first
    best_move = move_probs[0][0]
    # (create child node...)</code></pre>

                    <div class="checkpoint">
                        <h4>‚úÖ Phase 3 Validation Checklist</h4>
                        <p>Complete when:</p>
                        <ul class="checklist">
                            <li>Network trains without errors (loss decreases)</li>
                            <li>Policy network predicts legal moves >50% of time on held-out set</li>
                            <li>Value network correlates with game outcomes (Spearman œÅ > 0.5)</li>
                            <li>NN-MCTS matches hand-crafted MCTS strength (~1400-1600 Elo)</li>
                            <li>Inference is fast enough (~1-10ms per position)</li>
                            <li>No bugs in board encoding (visual inspection + tests)</li>
                        </ul>
                    </div>

                    <h3>What Comes Next: Phase 4 (Self-Play RL)</h3>

                    <p>In Phase 3, we train the network on <strong>human games</strong> (supervised learning). In Phase 4, we'll train on <strong>self-play games</strong> (reinforcement learning), allowing the engine to discover new strategies beyond human play!</p>

                    <div class="section-nav">
                        <a href="#phase2" class="prev">‚Üê Phase 2: MCTS</a>
                        <a href="#phase4" class="next">Next: Self-Play RL (Coming Soon) ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- PHASE 4: SELF-PLAY RL (COMING SOON) -->
            <details class="phase phase-coming-soon" id="phase4">
                <summary>üìç Phase 4: Self-Play Reinforcement Learning (Coming Soon)</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 4: Self-Play RL
                    </div>

                    <div class="day-marker">üìÖ Weeks 5-8 - Target: 1600-1800+ Elo</div>

                    <div class="learning-objective">
                        <h3>Planned Objectives - Phase 4</h3>
                        <p>Phase 4 will cover:</p>
                        <ul class="checklist-pending">
                            <li>Building self-play infrastructure</li>
                            <li>Replay buffer and experience storage</li>
                            <li>Training loop (self-play ‚Üí train ‚Üí evaluate ‚Üí promote)</li>
                            <li>Elo-based model promotion system</li>
                            <li>Continuous improvement through self-play</li>
                            <li>Discovering superhuman strategies</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: Learning from Self-Play</h3>

                    <p>Phase 4 is where the magic happens: the engine <strong>plays against itself</strong>, learns from its games, and continuously improves without human data!</p>

                    <div class="key-insight">
                        <strong>The Self-Play Loop:</strong>
                        <ol>
                            <li><strong>Generate:</strong> Play games using current network + MCTS</li>
                            <li><strong>Store:</strong> Save positions, MCTS policies, and game outcomes</li>
                            <li><strong>Train:</strong> Update network to match MCTS policies and predict outcomes</li>
                            <li><strong>Evaluate:</strong> Test new network against current best</li>
                            <li><strong>Promote:</strong> Replace current network if new one is stronger</li>
                            <li><strong>Repeat:</strong> Continuous improvement!</li>
                        </ol>
                    </div>

                    <p><em>Detailed tutorial for Phase 4 coming soon...</em></p>

                    <div class="section-nav">
                        <a href="#phase3" class="prev">‚Üê Phase 3: Neural Networks</a>
                        <a href="#phase5" class="next">Next: Scaling Up (Coming Soon) ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- PHASE 5: SCALING UP (COMING SOON) -->
            <details class="phase phase-coming-soon" id="phase5">
                <summary>üìç Phase 5: Scaling Up (Coming Soon)</summary>
                <div class="phase-content">
                    <div class="breadcrumb">
                        <a href="#intro">Home</a> ‚Üí Phase 5: Scaling Up
                    </div>

                    <div class="day-marker">üìÖ Weeks 9+ - Target: 1800+ Elo (Master Level)</div>

                    <div class="learning-objective">
                        <h3>Planned Objectives - Phase 5</h3>
                        <p>Phase 5 will cover:</p>
                        <ul class="checklist-pending">
                            <li>Larger neural networks (20-40 residual blocks)</li>
                            <li>More MCTS simulations (800-1600 per move)</li>
                            <li>Cloud compute for training (GPU clusters)</li>
                            <li>Distributed self-play workers</li>
                            <li>Advanced techniques (opening books, endgame tablebases)</li>
                            <li>Reaching master-level play (2000+ Elo)</li>
                        </ul>
                    </div>

                    <h3>üéØ Goal: Master-Level Chess AI</h3>

                    <p>Phase 5 is about <strong>scale</strong>: bigger networks, more compute, more games. This is where you can push toward master-level strength!</p>

                    <div class="key-insight">
                        <strong>Scaling Strategy:</strong>
                        <ul>
                            <li><strong>Start local:</strong> Mac mini M4 for self-play, cloud bursts for training</li>
                            <li><strong>Incremental scaling:</strong> Gradually increase network size and simulations</li>
                            <li><strong>Cost control:</strong> Short GPU bursts (2-6 hours) instead of 24/7 rentals</li>
                            <li><strong>Checkpoints persist:</strong> All progress carries forward (no wasted work)</li>
                        </ul>
                    </div>

                    <p><em>Detailed tutorial for Phase 5 coming soon...</em></p>

                    <div class="section-nav">
                        <a href="#phase4" class="prev">‚Üê Phase 4: Self-Play RL</a>
                        <a href="#glossary" class="next">Reference: Glossary ‚Üí</a>
                    </div>
                </div>
            </details>

            <!-- GLOSSARY -->
            <section id="glossary" style="margin-top: 60px;">
                <h2>üìñ Complete Glossary</h2>

                <div class="glossary-term">
                    <dt>Alpha-Beta Pruning</dt>
                    <dd>Optimization for minimax that skips searching branches that can't affect the final decision. With good move ordering, reduces nodes searched by ~95%.</dd>
                </div>

                <div class="glossary-term">
                    <dt>AlphaZero</dt>
                    <dd>DeepMind's 2017 chess/Go/shogi AI that uses self-play reinforcement learning with MCTS and neural networks. Achieved superhuman strength without human data.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Centipawn</dt>
                    <dd>Unit of chess evaluation. 100 centipawns = 1 pawn. Allows fine-grained position assessment (e.g., +20 = slight advantage worth 1/5 pawn).</dd>
                </div>

                <div class="glossary-term">
                    <dt>Elo Rating</dt>
                    <dd>Chess strength measurement system. ~1200 = beginner, ~1600 = intermediate, ~2000 = expert, ~2400 = master, ~2800 = world champion.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Evaluation Function</dt>
                    <dd>Function that assigns a numerical score to a chess position. Positive = White winning, negative = Black winning, zero = equal.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Horizon Effect</dt>
                    <dd>Bug in fixed-depth search where engine stops searching right before a tactical blow. Solved by quiescence search.</dd>
                </div>

                <div class="glossary-term">
                    <dt>MCTS (Monte Carlo Tree Search)</dt>
                    <dd>Search algorithm that uses statistical sampling instead of exhaustive search. Balances exploration vs exploitation using UCT formula.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Minimax</dt>
                    <dd>Classic game tree search algorithm. Maximizing player tries to maximize score, minimizing player tries to minimize, both assuming optimal play.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Policy Network</dt>
                    <dd>Neural network that outputs probability distribution over moves. Tells MCTS which moves to explore first.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Quiescence Search</dt>
                    <dd>Extension of minimax that continues searching tactical moves (captures, checks) until position is quiet. Eliminates horizon effect.</dd>
                </div>

                <div class="glossary-term">
                    <dt>ResNet (Residual Network)</dt>
                    <dd>Neural network architecture with skip connections. Enables training very deep networks (40+ layers) without vanishing gradients.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Rollout</dt>
                    <dd>In MCTS, playing out a game from a position to estimate its value. Can use random moves, evaluation function, or neural network.</dd>
                </div>

                <div class="glossary-term">
                    <dt>Self-Play</dt>
                    <dd>Training technique where AI plays against itself to generate training data. Key to AlphaZero's superhuman performance.</dd>
                </div>

                <div class="glossary-term">
                    <dt>UCI (Universal Chess Interface)</dt>
                    <dd>Standard text-based protocol for chess engines and GUIs. Allows any UCI engine to work with any UCI GUI (Arena, Cute Chess, etc.).</dd>
                </div>

                <div class="glossary-term">
                    <dt>UCT (Upper Confidence Bound for Trees)</dt>
                    <dd>Formula for node selection in MCTS. Balances exploitation (try winning moves) with exploration (try less-visited moves).</dd>
                </div>

                <div class="glossary-term">
                    <dt>Value Network</dt>
                    <dd>Neural network that outputs position evaluation in [-1, +1]. Tells MCTS how good a position is without rollouts.</dd>
                </div>
            </section>

            <!-- RESOURCES -->
            <section id="resources" style="margin-top: 60px;">
                <h2>üìö Learning Resources</h2>

                <h3>Chess Programming</h3>
                <ul>
                    <li><strong>Chess Programming Wiki:</strong> <code>https://www.chessprogramming.org</code> - Comprehensive wiki on chess engine development</li>
                    <li><strong>python-chess Documentation:</strong> <code>https://python-chess.readthedocs.io</code> - Library used in this project</li>
                    <li><strong>Cute Chess:</strong> GUI for testing engines and running tournaments</li>
                </ul>

                <h3>Algorithms</h3>
                <ul>
                    <li><strong>Minimax & Alpha-Beta:</strong> Classic AI textbooks (Russell & Norvig, "Artificial Intelligence: A Modern Approach")</li>
                    <li><strong>Monte Carlo Tree Search:</strong> "A Survey of Monte Carlo Tree Search Methods" (Browne et al., 2012)</li>
                    <li><strong>UCT Algorithm:</strong> "Bandit Based Monte-Carlo Planning" (Kocsis & Szepesv√°ri, 2006)</li>
                </ul>

                <h3>Neural Networks & Deep Learning</h3>
                <ul>
                    <li><strong>AlphaZero Paper:</strong> "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" (Silver et al., 2017)</li>
                    <li><strong>ResNet Paper:</strong> "Deep Residual Learning for Image Recognition" (He et al., 2015)</li>
                    <li><strong>PyTorch Tutorials:</strong> <code>https://pytorch.org/tutorials</code> - Official PyTorch learning resources</li>
                </ul>

                <h3>Reinforcement Learning</h3>
                <ul>
                    <li><strong>Sutton & Barto:</strong> "Reinforcement Learning: An Introduction" (classic RL textbook)</li>
                    <li><strong>Spinning Up in Deep RL:</strong> OpenAI's educational resource on deep RL</li>
                    <li><strong>AlphaGo Documentary:</strong> Excellent introduction to the concepts behind AlphaZero</li>
                </ul>
            </section>

            <!-- ALPHAZERO TIMELINE -->
            <section id="timeline" style="margin-top: 60px; margin-bottom: 60px;">
                <h2>üïê AlphaZero: Historical Timeline</h2>

                <table>
                    <tr>
                        <th>Year</th>
                        <th>Milestone</th>
                        <th>Impact</th>
                    </tr>
                    <tr>
                        <td>2016</td>
                        <td>AlphaGo defeats Lee Sedol (Go world champion)</td>
                        <td>First superhuman Go AI</td>
                    </tr>
                    <tr>
                        <td>2017</td>
                        <td>AlphaGo Zero learns from pure self-play</td>
                        <td>Surpasses AlphaGo without human data</td>
                    </tr>
                    <tr>
                        <td>Dec 2017</td>
                        <td>AlphaZero generalizes to chess, shogi, Go</td>
                        <td>Defeats Stockfish (strongest chess engine)</td>
                    </tr>
                    <tr>
                        <td>2018-Present</td>
                        <td>Open-source implementations (Leela Chess Zero, KataGo)</td>
                        <td>AlphaZero techniques democratized</td>
                    </tr>
                </table>

                <div class="key-insight">
                    <strong>What AlphaZero Taught Us:</strong>
                    <ul>
                        <li>Self-play RL can achieve superhuman performance without human data</li>
                        <li>General algorithms work across different games (chess, Go, shogi)</li>
                        <li>Neural networks can discover novel strategies humans never found</li>
                        <li>Combining search (MCTS) with learning (neural networks) is incredibly powerful</li>
                    </ul>
                </div>
            </section>

        </div>
    </main>

    <!-- JAVASCRIPT FOR NAVIGATION -->
    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });

                    // Update sidebar active state
                    document.querySelectorAll('.sidebar nav ul li').forEach(li => {
                        li.classList.remove('current');
                    });
                    this.closest('li')?.classList.add('current');
                }
            });
        });

        // Open phase based on URL hash
        window.addEventListener('load', function() {
            const hash = window.location.hash;
            if (hash) {
                const target = document.querySelector(hash);
                if (target && target.tagName === 'DETAILS') {
                    target.open = true;
                    target.scrollIntoView({ behavior: 'smooth' });
                }
            }
        });

        // Update hash when opening/closing phases
        document.querySelectorAll('details.phase').forEach(phase => {
            phase.addEventListener('toggle', function() {
                if (this.open) {
                    window.location.hash = this.id;
                }
            });
        });
    </script>
</body>
</html>
